\chapter{Results and Perspective}
Once that the features used and the classification method (SVM algorithm) are defined, we run the pipeline on our \emph{gold data}. In this part we'll show the results returned by DKPro and we'll try to comment them. Then with the error analysis, we'll try to understand why our classification method has still some issues to recognize certain instances (a machine learning method would have some issues to understand \emph{irony} or \emph{sarcasm}). We'll also determine the features that contain high information thank to feature selection methods. Finally we'll have a look at extra tasks that should be done in the future.
\section{Performance Evaluation}
In this section, we display the macro F-measure for 11 different sets of features. Except if it's indicated, for every row, we consider the feature that is written in the cell in addition with the previous ones (for example for \emph{POS Ratio}, we consider \emph{POS Ratio} features and \emph{Baseline} features).
\\
\textbf{Full Cross-Validation and Full Cross-Domain}
\begin{table}[h]
\center
\begin{tabular}{|l|l|l|}
\hline
Features                           & Full CV                                              & Fm Full CD                                           \\ \hline
Baseline                           & \cellcolor[HTML]{656565}{\color[HTML]{000000} 68.92} & 60.07                                                \\ \hline
Only Lemma NGrams                       & 63.81                                                & 58.32                                                \\ \hline
POS Ratio                          & 68.83                                                & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} 60.75} \\ \hline
1-Skip 2-grams                     & 65.55                                                & 55.52                                                \\ \hline
POS NGrams + Sub-clause Ratio      & 64.90                                                & 57.91                                                \\ \hline
Sentiment Analysis                 & 67.80                                                & 60.32                                                \\ \hline
Only Baseline + Sentiment Analysis & 67.70                                                & 60.30                                                \\ \hline
Sentiment Fluctuation              & 67.61                                                & 60.21                                                \\ \hline
30 Dependency Rules                & 67.60                                                & 58.51                                                \\ \hline
5000 Dependency Rules              & 66.73                                                & \cellcolor[HTML]{656565}60.90                        \\ \hline
LDA                                & \cellcolor[HTML]{C0C0C0}68.12                        & 60.22                                                \\ \hline
\end{tabular}
\end{table}

The best result is highlighted in dark grey and the second best result is highlighted in light grey.

This table shows us that the different configurations are more or less equivalent (indeed, the confidence intervals overlap in most of the cases) which mean that the Baseline performs as good as fancier methods. We need to investigate the results for the in-domain cross-validation and the cross-domain validation to see if this trend is going on.
\\
\\
\textbf{In-Domain Cross-Validation}
\\
It should be reminded that for the mainstreaming domain, a leave-one-out cross-validation is performed instead of a 10-folds CV.
\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Features                           & RS                                                   & PIS                           & HS                            & SSE                           & MS                            & PPS                           \\ \hline
Baseline                           & 51.37                                                & 73.94                         & 70.53                         & 65.32                         & 39.58                         & 68.5                          \\ \hline
Lemma NGrams                       & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} 55.89} & 71.23                         & 67.35                         & \cellcolor[HTML]{C0C0C0}74.0  & \cellcolor[HTML]{656565}43.50 & 61.71                         \\ \hline
POS Ratio                          & 52.78                                                & \cellcolor[HTML]{C0C0C0}74.62 & 70.13                         & 65.32                         & 39.58                         & 70.25                         \\ \hline
1-Skip 2-grams                     & 45.29                                                & 64.34                         & 70.10                         & 75.37                         & 39.58                         & 68.57                         \\ \hline
POS NGrams + Sub-clause Ratio      & \cellcolor[HTML]{656565}59.52                        & 69.78                         & 70.09                         & \cellcolor[HTML]{656565}77.92 & 39.58                         & 66.90                         \\ \hline
Sentiment Analysis                 & 54.16                                                & 73.32                         & \cellcolor[HTML]{C0C0C0}71.55 & 67.53                         & 39.58                         & \cellcolor[HTML]{C0C0C0}70.57 \\ \hline
Only Baseline + Sentiment Analysis & 52.78                                                & 72.64                         & \cellcolor[HTML]{C0C0C0}71.55 & 65.32                         & 39.58                         & 70.00                         \\ \hline
Sentiment Fluctuation              & 52.78                                                & \cellcolor[HTML]{656565}75.35 & \cellcolor[HTML]{656565}72.93 & 69.70                         & 39.58                         & \cellcolor[HTML]{656565}71.26 \\ \hline
5000 Dependency Rules              & 49.60                                                & 73.79                         & 68.11                         & 73.91                         & 39.58                         & 68.85                         \\ \hline
LDA                                & 50.95                                                & 60.22                         & 69.73                         & 73.91                         & 39.58                         & 68.66                         \\ \hline
\end{tabular}
\end{table}
\\
\\
\textbf{Cross-Domain Validation}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Features                           & RS                                                   & PIS                           & HS                            & SSE                           & MS                            & PPS                           \\ \hline
Baseline                           & 54.16                                                & 52.62                         & 61.02                         & 62.50                         & \cellcolor[HTML]{C0C0C0}61.84 & \cellcolor[HTML]{C0C0C0}65.90 \\ \hline
Lemma NGrams                       & 51.38                                                & 50.10                         & 60.36                         & 60.73                         & 57.35                         & 62.00                         \\ \hline
POS Ratio                          & 54.16                                                & \cellcolor[HTML]{C0C0C0}53.23 & \cellcolor[HTML]{656565}62.67 & 60.07                         & \cellcolor[HTML]{C0C0C0}61.84 & \cellcolor[HTML]{656565}66.27 \\ \hline
1-Skip 2-grams                     & 48.89                                                & 51.79                         & 58.34                         & 47.92                         & 38.75                         & 62.44                         \\ \hline
POS NGrams + Sub-clause Ratio      & 51.21                                                & 48.91                         & 55.60                         & \cellcolor[HTML]{C0C0C0}65.65 & \cellcolor[HTML]{656565}63.39 & 62.58                         \\ \hline
Sentiment Analysis                 & 55.53                                                & 53.02                         & 61.17                         & 60.07                         & \cellcolor[HTML]{C0C0C0}61.84 & 64.83                         \\ \hline
Only Baseline + Sentiment Analysis & 55.53                                                & 52.39                         & 60.91                         & 60.07                         & 58.95                         & 65.81                         \\ \hline
Sentiment Fluctuation              & 51.20                                                & \cellcolor[HTML]{656565}53.44 & 62.16                         & 62.50                         & 48.73                         & 64.10                         \\ \hline
5000 Dependency Rules              & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} 59.87} & 53.84                         & \cellcolor[HTML]{C0C0C0}62.40 & \cellcolor[HTML]{656565}67.79 & 48.73                         & 64.21                         \\ \hline
LDA                                & \cellcolor[HTML]{656565}61.46                        & 53.22                         & \cellcolor[HTML]{C0C0C0}62.40 & 63.47                         & 51.48                         & 63.39                         \\ \hline
\end{tabular}
\end{table}

Some significant improvements can be seen for the domains redshirting, single sex education, and public private schools, but not for the same feature sets configurations. For example, if we consider the in-domain cross-validation, sentiment analysis works pretty well for prayer in schools and homeschooling. Those debates are very sensible and lead to highly sentimental data, such as follow:
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{1279\_P2\_artcomment\_prayer-in-schools.txt} I love the posters that always claim religion is the route of all evil, violence and strife. Yet socialism/communism are responsible for up to 200 million dead leading up to the 21st century, and still counting. 
All the religious wars throughout human history don't even come close to these atheist murderers! 
The one parameter that all the accusers fail to recognize, is that they are all run by Humans. 
There is something seriously wrong with Homo Sapiens.
\end{minipage}}
\\

Regarding the cross-domain scenarios, it seems that \emph{LDA} and dependencies rules are the two techniques which work the best and thus give the best features which can detect persuasiveness among different domains. This is due to the fact that argumentation structures tend to be similar and thus dependency rules and the graphs of topics (for the LDA) tend to be the same.

To better understand our actual results, we'll perform an analysis on the feature by selecting the ones which gather most of the information: it's \emph{Feature Selection}.  


\section{Feature Selection}
We consider the case where we take all the features into account (except the POS N-Grams and the Lemmas). Here is an overview of the types of features we have and their distribution. 
\\
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Type of Feature      & Number \\ \hline
Baseline             & 10000  \\ \hline
Part of Speech (POS) & 16     \\ \hline
Lexical              & 15     \\ \hline
Syntactic            & 5010   \\ \hline
Sentiment            & 45     \\ \hline
LDA                  & 30     \\ \hline
\rowcolor[HTML]{C0C0C0} 
TOTAL                & 15116  \\ \hline
\end{tabular}
\end{table}

Even before the selection, we see a clear domination of baseline features which corresponds to n-grams (with n=1,2,3) presence in a text. The high number of syntactic features is due to the 5000 dependency rules that are extracted from the corpus. 
\\
\\
\textbf{Information Gain}
\\
Information Gain is a measure that evaluates the worth of an attribute by measuring an entropy difference. Information Gain is calculated such as follows:
\begin{equation*}
InfoGain(Class,Feature) = H(Class) - H(Class | Feature)
\end{equation*}

Where $H$ is the information entropy which in our case (two classes P1 and P2) is written as:
\begin{equation*}
H(Class) = -p(P1) \: log \: p(P1) - p(P2) \: log \: p(P2)
\end{equation*}
\begin{equation*}
H(Class | Feature) = p(P1, Feature) \: \: log \: \frac{p(Feature)}{p(P1, Feature)} - p(P2, Feature) \: log \: \frac{p(Feature)}{p(P2, Feature)}
\end{equation*}

p is here the probability. \emph{Feature} corresponds here to any of the 15116 features we extracted from the text corpus. Thank to the function \emph{InfoGainAttributeEval} available in Weka, 15116 information gains are computed and the features are ranked from the highest information gain to the lowest.

For the full cross-validation scenario, here are the 20 ``best" features we obtain: 

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
Rank & Feature                                                                                                                         & Information Gain \\ \hline
1    & post length                                                                                                                     & 0.10607          \\ \hline
2    & number of sentences                                                                                                             & 0.07802          \\ \hline
3    & ngram \textbf{in}                                                                                                             & 0.07338          \\ \hline
4    & ngram \textbf{school}                                                                                                         & 0.06966          \\ \hline
5    & ngram \textbf{public}                                                                                                         & 0.06615          \\ \hline
6    & ngram \textbf{private}                                                                                                        & 0.06578          \\ \hline
7    & \begin{tabular}[c]{@{}l@{}}LDA school private send public bid \\ afford sacrifice wealthy rich crappy\end{tabular}              & 0.06558          \\ \hline
8    & \begin{tabular}[c]{@{}l@{}}LDA education system public change improve\\ quality educational voucher author product\end{tabular} & 0.06377          \\ \hline
9    & sentiment very positive standard deviation                                                                                      & 0.06094          \\ \hline
10   & \begin{tabular}[c]{@{}l@{}}LDA year start back hold grade\\ age kindergarten ready son turn\end{tabular}                        & 0.06089          \\ \hline
11   & max number of Sub-Clauses                                                                                                       & 0.05945          \\ \hline
12   & sentiment very negative standard deviation                                                                                      & 0.05933          \\ \hline
13   & \begin{tabular}[c]{@{}l@{}}LDA boy girl learn sex single \\ woman classroom young experience gender\end{tabular}                & 0.0559           \\ \hline
14   & sentiment negative standard deviation                                                                                           & 0.0554           \\ \hline
15   & cardinal ratio                                                                                                                  & 0.05313          \\ \hline
16   & \begin{tabular}[c]{@{}l@{}}LDA teacher teach school union job\\ classroom good run hand ca\end{tabular}                         & 0.05309          \\ \hline
17   & dependency rule S \rightarrow NP \rightarrow VP                                                                                   & 0.05282          \\ \hline
18   & ngram \textbf{schools}                                                                                                        & 0.05165          \\ \hline
19   & sentiment positive standard deviation                                                                                           & 0.05127          \\ \hline
20   & ngram \textbf{to}                                                                                                             & 0.05026          \\ \hline
\end{tabular}
\end{table}

Topic related features (ngrams and LDA) are here obviously dominant. Indeed, the available data mostly deal with school, education and religion. Even so, some general features (syntactic, lexical, sentiment) are also present. Regarding lexical features, the 2 first best are related to the length of a post (global length in characters and number of sentences). For sentiments, the ones that count are the standard deviations.

Now we remove all the features with an information gain equals to 0. There are 1738 remaining features, with the following distribution:
\\
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
Type of Feature      & Number \\ \hline
Baseline             & 1476  \\ \hline
Part of Speech (POS) & 11     \\ \hline
Lexical              & 10     \\ \hline
Syntactic            & 185   \\ \hline
Sentiment            & 31     \\ \hline
LDA                  & 25     \\ \hline
\rowcolor[HTML]{C0C0C0} 
TOTAL                & 1738  \\ \hline
\end{tabular}
\end{table}

The features that we engineered are quite pertinent since 70\% of the POS, 67\% of the lexical, 68\% of the sentiment and 80\% of the LDA features have an information gain strictly higher than 0. Even so, the baseline n-grams features still represent 85\% which means that 85\% of the information that tell us if a text is P1 or P2 is contained in simple topic-related and uneasily interpretable features.

\section{Error Analysis}
In the \cref{sec:conflictannotations}, we described how in the annotation process, some of the posts are declared as \emph{conflicting} since the annotators didn't unanimously agree on its class. In following example, for one of the annotator, the post is persuasive, the author implicitly supports homeschooling in the sentence \emph{many are willing to ignore several centuries of improvements in organized education in our part of the world}, but the two other annotators wrote that they were not able to conclude without any extra knowledge.
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{329\_P2\_artcomment\_homeschooling.txt} The comments are more fascinating than the article. Many folks, some of them with impressive intellectual accomplishments, would have us believe that the current era is different, that we, especially in the industrialized world, have moved beyond superstition into truly rational ways of thinking. The comments prove quite the opposite. Many people firmly believe things obviously at odds with reality. In particular, many are willing to ignore several centuries of improvements in organized education in our part of the world. 
In addition, many people refuse to accept that we can do things as a group, "society" that is, that we can't do as individuals. This does not bode well for our future, since in a very real sense we are all stuck with each other. 
\end{minipage}}
\\

To find out if our classifier also ``hesitates" on conflicting posts, we define 4 coefficients. By analogy with the binary classification, we call them TP, TN, FP and FN:
\\

\texttt{TP: Correctly classified and no conflict}

\texttt{TN: Misclassified and conflict }

\texttt{FP: Correctly classified and conflict}

\texttt{FN: Misclassified but no conflict}  
\\

Again, the coefficients are named this way just by analogy with binary classification, but here they don't record the performances of a classification. Similarly, they can be represented in a confusion matrix:
\\
\\
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
              & No Conflict                & Conflict \\ \hline
Classified    & TP                         & FP       \\ \hline
Misclassified & \cellcolor[HTML]{9B9B9B}FN & TN       \\ \hline
\end{tabular}
\end{table}
\\

The FN cell, highlighted in gray, is the most problematic one. It corresponds to the cases where the annotators unanimously agreed on the class but the classifier couldn't recognize it. To quantify how much the classifier can't recognize those instances we decided to compute the following coefficient:
\begin{equation*}
\frac{TP + TN + FP}{TP + FN + FP + \textbf{FN}}
\end{equation*}

We obtain the following results for the different configurations:
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
In-Domain   & Full  & RS    & PIS   & HS    & SSE   & MS    & PPS   \\ \hline
Coefficient & 73.64 & 57.35 & 81.82 & 77.68 & 74.00 & 68.97 & 78.57 \\ \hline
\end{tabular}
\end{table}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Cross-Domain & RS    & PIS   & HS    & SSE   & MS    & PPS   \\ \hline
Coefficient  & 61.76 & 62.24 & 70.54 & 70.00 & 62.07 & 78.57 \\ \hline
\end{tabular}
\end{table}
\\
To conclude this error analysis, we'll pick-up randomly 5 \emph{FN} cases out of the 261 ones of the full cross-validation and try to analyse why they were misclassified.
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{3073\_P2\_artcomment\_public-private-schools.txt} Parenting styles & decisions, including where to send your children to school, is a very subjective matter.  Saying that someone is a bad person for sending their children to private/parochial school is no different than someone telling me that I'm not a whole or complete woman because I have chosen not to have any children. Ridiculous. 
\end{minipage}}
\\
\\
It's hard to assign the label P1 to this post without deep knowledge. That said, the annotators considered it as P2. But here, the length of the sentences (quite long), the diversity of the vocabulary and a sentiment tone which stays more or less neutral, made the classifier to conclude P1 for the predicted class.
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{323\_P1\_artcomment\_homeschooling.txt} There are the rare exceptions where homeschooling is great - but for most it is child abuse.
Regular testing should be required and if they fail it's back to public school.
\end{minipage}}
\\
\\
Here it's totally the opposite. The author clearly shows his views about homeschooling - \emph{There are the rare exceptions where homeschooling is great} - but the post is relatively short and there's high sentiment variation (\emph{is great} and \emph{child abuse}), that's the reason why the classifier classified it as non persuasive.
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{1617\_P2\_forumpost\_prayer-in-schools.txt} I don't believe that anyone is saying that students shouldn't be allowed to pray in school... in the form of an individual saying grace over lunch or before the start of a test... 
The issue is with official prayers read over the loudspeaker or otherwise directly sanctioned by school authorities. 
\end{minipage}}
\\
\\
Here, there's quite a lot of topic related vocabulary: students, pray, school, ect... and besides some argumentative forms are present, \emph{I don't believe} and \emph{students shouldn't}. This could explain why the classifier considered it as P1.
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{4891\_P2\_forumpost\_mainstreaming.txt} Hi,,
For the past 2 years my dd has been at trinity independent school, I. Rochester. It's a social school for kiddies with asd, dyslexia, dyscalclia and motor probs. Well we were self funding her due to her not being able to be statmented due to lack of funding * rolls eyes * . We got into some financial probs and the school have asked us not to return her in Jan due to the amount that's owed by us. We have tried 17 schools in Medway to try and see if they will take my dd, all have refused due to her not being statemented and them not being able to meet her needs. The lea have told me to retry for a statement and to keep dd at home until it comes thru, however this takes 6 months lol!! I really don't know what to do as I'm a full time uni student and work, I can't homeschool her. Plus her sensory needs need addressing in a strict school routine and m scared for her being taken out of this. To top things off the school knew damn well we would struggle to find her an appropriate school due to her needs, but still said they would happily have her back once statemented. I'm lost,, can anyone help me with what to do?? 
Sam xx
\end{minipage}}
\\
\\
The author is asking for advice and clearly don't show any aim to persuade.  Nevertheless, the post is very long, so are the sentences and despite the grammar mistakes, there's a lot of topic related vocabulary.
\\
\\
\fbox{\begin{minipage}{1\textwidth}
\textbf{5145\_P1\_forumpost\_redshirting.txt} I know several people who said that they wish their parents had waited a year to send them to kindergarten. DH was 4 when he started school and was only 17 when he graduated from HS. He hated that everyone was doing everything a year before he could do it. He was very good at sports though, even though he was a year younger than anyone else on his team. 
I honestly made my decision because I had a choice and I just did not feel right personally about sending my DS to Kindergarten at age 4. Last year the cut off was October 9th I believe and this year it has been changed to July 1st. DS's birthday is August 30th. 
I did not wait to send DS because I wanted him to have an edge over anyone. Age has nothing to do with ability and if he were the type who would struggle, he would struggle regardless of when he started school. I do not compare my children with other people's children. I do what is best for my children and assume other parents do what is best for their children. 
Age wise I was right in the middle of my class at school and I was far ahead of my class. I don't think age really dictates how well you will do in school or in sports. 
\end{minipage}}
\\
\\
Lot of sentiment fluctuation but above all lots of occurrences of the first person.
\\

Here are the results of the cross-validations if we take out the conflicting instances:
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
In Domain CV             & Full  & RS    & PIS   & HS    & SSE   & MS    & PPS   \\ \hline
Conflicts removed        & 75.24 & 65.22 & 80.17 & 82.66 & 61.36 & 68.42 & 77.71 \\ \hline
With conflict (reminder) & 68.18 & 51.48 & 74.13 & 73.21 & 74.00 & 65.52 & 70.59 \\ \hline
\end{tabular}
\end{table}
Here are shown the accuracies. There's a significant increase for all the domains except mainstreaming and single sex education that can be explained by the low quantity of data available for those two domains.
\section{Future Work and General Conclusion}
The different results we obtained lead us to think that other techniques can be applied to this classification problem. Obviously, more lexical and syntactic features should be implemented. For example, if I had more time, I could have integrated the Ruby-based Penn Discourse Tree Bank\cite{NLE:9163968} parser\footnote{http://wing.comp.nus.edu.sg/~linzihen/parser} in DKPro which would have given us discourse related feature sets. 

Besides, in this study, meaningful features are highly topic-related and this because the domains distribution is not equal (476 texts for public private schools debate and only 29 for mainstreaming). If we want to gain in generality regarding our features, we should extend the quantity of domains. Moreover, 990 texts might be considered as low for a classification problem, but how is it possible to extend the dataset without launching another annotation phase which is time time-consuming? We came with the idea of \emph{bootstrapping} our training set by parsing some debate portal and consider all the posts as P1 (if the amount of data extracted is high enough, this approximation is acceptable since most of the posts of a debate portal should contain persuasiveness). We already have a content extractor written in Java using \emph{jsoup}\footnote{jsoup is a Java library for working with real-world HTML, http://jsoup.org/} that can extract posts from the debate portal createdebate\footnote{http://www.createdebate.com/}. This portal has a \emph{Creative Commons License} so we can freely use its content. As of today, bootstrapping was not launched, but it should be possible to do it soon.

\section{Personal Conclusion}
This internship in the field of NLP and text mining gave me an opportunity to understand the way research works. Working in this environment taught me how to organize myself (in terms of programming, research, ect...) and how to collaborate with others.
